{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fefda351",
   "metadata": {},
   "source": [
    "# ðŸ”¬ LLM Sensitivity & Super-Weight Prioritization Analysis\n",
    "\n",
    "**Research Objective**: Identify and analyze critical weights in large language models through gradient-based sensitivity metrics.\n",
    "\n",
    "## ðŸ“‹ Analysis Pipeline:\n",
    "1. **Model Loading**: Load target model with proper tokenization\n",
    "2. **Baseline Evaluation**: Compute initial perplexity\n",
    "3. **Sensitivity Computation**: Calculate gradient-based metrics\n",
    "4. **Weight Ranking**: Identify top-K critical weights\n",
    "5. **Perturbation Analysis**: Mask weights and evaluate impact\n",
    "6. **Results Export**: Save findings for further analysis\n",
    "\n",
    "## ðŸŽ¯ Key Metrics:\n",
    "- **Gradient Ã— Weight**: `|grad Ã— weight|`\n",
    "- **GradientÂ²**: `gradÂ²` (curvature-based sensitivity)\n",
    "- **Top-K Analysis**: Most critical weights by layer\n",
    "- **Impact Assessment**: Perplexity change after masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a12f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Project imports\n",
    "sys.path.append('../src')\n",
    "from models.loader import load_model\n",
    "from eval.perplexity import compute_perplexity\n",
    "from sensitivity.metrics import compute_sensitivity, get_model_layers\n",
    "from sensitivity.rank import rank_weights_topk\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"ðŸ”¬ Critical Weight Analysis - Research Notebook\")\n",
    "print(f\"ðŸ“… Session: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"ðŸ”§ PyTorch: {torch.__version__}\")\n",
    "print(f\"ðŸŽ¯ CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"ðŸš€ GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"ðŸ’¾ Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c47f394",
   "metadata": {},
   "source": [
    "## âš™ï¸ Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23acf811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment configuration\n",
    "CONFIG = {\n",
    "    # Model settings\n",
    "    'model_name': 'gpt2',  # Start with GPT-2, can change to EleutherAI/pythia-410m\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    \n",
    "    # Analysis settings\n",
    "    'sensitivity_metrics': ['grad_weight', 'grad_squared'],\n",
    "    'topk_values': [10, 50, 100, 500],  # Top-K weights to analyze\n",
    "    'perturbation_ratios': [0.1, 0.25, 0.5, 0.75, 1.0],  # Fraction of top-K to mask\n",
    "    \n",
    "    # Data settings\n",
    "    'eval_texts_limit': 100,  # Number of evaluation texts\n",
    "    'batch_size': 1,  # Individual processing for accuracy\n",
    "    \n",
    "    # Output settings\n",
    "    'save_results': True,\n",
    "    'output_dir': '../outputs',\n",
    "    'timestamp': datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "}\n",
    "\n",
    "print(\"ðŸ“Š Experiment Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path(CONFIG['output_dir'])\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "session_dir = output_dir / f\"sensitivity_analysis_{CONFIG['timestamp']}\"\n",
    "session_dir.mkdir(exist_ok=True)\n",
    "print(f\"\\nðŸ’¾ Results will be saved to: {session_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86bc935",
   "metadata": {},
   "source": [
    "## ðŸ“š Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5a5285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation texts\n",
    "data_file = Path('../src/data/dev_small.txt')\n",
    "if data_file.exists():\n",
    "    with open(data_file, 'r') as f:\n",
    "        all_texts = [line.strip() for line in f if line.strip()]\n",
    "else:\n",
    "    # Fallback: create sample texts\n",
    "    all_texts = [\n",
    "        \"The quick brown fox jumps over the lazy dog.\",\n",
    "        \"Machine learning models require careful optimization.\",\n",
    "        \"Natural language processing has advanced significantly.\",\n",
    "        \"Deep neural networks learn complex patterns.\",\n",
    "        \"Gradient-based methods are fundamental to training.\"\n",
    "    ] * 20  # Repeat to get more samples\n",
    "\n",
    "# Limit texts for efficiency\n",
    "eval_texts = all_texts[:CONFIG['eval_texts_limit']]\n",
    "\n",
    "print(f\"ðŸ“š Loaded {len(eval_texts)} evaluation texts\")\n",
    "print(f\"ðŸ“ Sample text: {eval_texts[0][:100]}...\")\n",
    "print(f\"ðŸ“ Average length: {np.mean([len(text.split()) for text in eval_texts]):.1f} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2045c3",
   "metadata": {},
   "source": [
    "## ðŸ¤– Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50572371",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ðŸ”„ Loading model: {CONFIG['model_name']}\")\n",
    "model, tokenizer = load_model(CONFIG['model_name'], device=CONFIG['device'])\n",
    "\n",
    "print(f\"âœ… Model: {type(model).__name__}\")\n",
    "print(f\"âœ… Tokenizer: {type(tokenizer).__name__}\")\n",
    "print(f\"âœ… Device: {next(model.parameters()).device}\")\n",
    "print(f\"ðŸ“Š Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"ðŸ” Trainable: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# Analyze model structure\n",
    "layers = get_model_layers(model)\n",
    "print(f\"\\nðŸ—ï¸ Model Architecture:\")\n",
    "print(f\"  Total layers: {len(layers)}\")\n",
    "for i, layer_name in enumerate(layers[:5]):  # Show first 5\n",
    "    layer = dict(model.named_modules())[layer_name]\n",
    "    if hasattr(layer, 'weight') and layer.weight is not None:\n",
    "        print(f\"  [{i}] {layer_name}: {tuple(layer.weight.shape)}\")\n",
    "if len(layers) > 5:\n",
    "    print(f\"  ... and {len(layers) - 5} more layers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6348628c",
   "metadata": {},
   "source": [
    "## ðŸ“ Baseline Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c7e0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ“ Computing baseline perplexity...\")\n",
    "baseline_ppl = compute_perplexity(model, tokenizer, eval_texts)\n",
    "\n",
    "print(f\"\\nðŸ“Š Baseline Results:\")\n",
    "print(f\"  Perplexity: {baseline_ppl:.2f}\")\n",
    "print(f\"  Log Perplexity: {np.log(baseline_ppl):.4f}\")\n",
    "print(f\"  Evaluation texts: {len(eval_texts)}\")\n",
    "\n",
    "# Store baseline for comparison\n",
    "results = {\n",
    "    'baseline_perplexity': baseline_ppl,\n",
    "    'model_name': CONFIG['model_name'],\n",
    "    'num_parameters': sum(p.numel() for p in model.parameters()),\n",
    "    'eval_texts_count': len(eval_texts)\n",
    "}\n",
    "\n",
    "print(f\"\\nâœ… Baseline established: PPL = {baseline_ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab33eaf",
   "metadata": {},
   "source": [
    "## ðŸ§® Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f65d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ§® Computing gradient-based sensitivity metrics...\")\n",
    "print(f\"ðŸ“Š Metrics: {', '.join(CONFIG['sensitivity_metrics'])}\")\n",
    "\n",
    "# Compute sensitivity for each metric\n",
    "sensitivity_results = {}\n",
    "\n",
    "for metric in CONFIG['sensitivity_metrics']:\n",
    "    print(f\"\\nðŸ”„ Computing {metric} sensitivity...\")\n",
    "    \n",
    "    # Calculate sensitivity\n",
    "    layer_sensitivities = compute_sensitivity(\n",
    "        model, tokenizer, eval_texts, \n",
    "        method=metric,\n",
    "        device=CONFIG['device']\n",
    "    )\n",
    "    \n",
    "    sensitivity_results[metric] = layer_sensitivities\n",
    "    \n",
    "    # Summary statistics\n",
    "    total_weights = sum(len(sens) for sens in layer_sensitivities.values())\n",
    "    all_sensitivities = np.concatenate([sens.cpu().numpy().flatten() \n",
    "                                       for sens in layer_sensitivities.values()])\n",
    "    \n",
    "    print(f\"  âœ… Computed for {len(layer_sensitivities)} layers\")\n",
    "    print(f\"  ðŸ“Š Total weights: {total_weights:,}\")\n",
    "    print(f\"  ðŸ“ˆ Mean sensitivity: {all_sensitivities.mean():.6f}\")\n",
    "    print(f\"  ðŸ“‰ Std sensitivity: {all_sensitivities.std():.6f}\")\n",
    "    print(f\"  ðŸ” Max sensitivity: {all_sensitivities.max():.6f}\")\n",
    "\n",
    "print(f\"\\nâœ… Sensitivity analysis complete for {len(CONFIG['sensitivity_metrics'])} metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcdc89d",
   "metadata": {},
   "source": [
    "## ðŸ† Weight Ranking & Top-K Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a496c4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ† Ranking weights by sensitivity...\")\n",
    "\n",
    "# Store ranking results\n",
    "ranking_results = {}\n",
    "\n",
    "for metric in CONFIG['sensitivity_metrics']:\n",
    "    print(f\"\\nðŸ”„ Ranking for {metric}...\")\n",
    "    \n",
    "    layer_sensitivities = sensitivity_results[metric]\n",
    "    \n",
    "    # Rank weights globally\n",
    "    topk_results = {}\n",
    "    \n",
    "    for k in CONFIG['topk_values']:\n",
    "        top_weights = rank_weights_topk(layer_sensitivities, k=k)\n",
    "        topk_results[k] = top_weights\n",
    "        \n",
    "        # Analyze distribution across layers\n",
    "        layer_counts = {}\n",
    "        for weight_info in top_weights:\n",
    "            layer = weight_info['layer']\n",
    "            layer_counts[layer] = layer_counts.get(layer, 0) + 1\n",
    "        \n",
    "        print(f\"  Top-{k}: {len(top_weights)} weights across {len(layer_counts)} layers\")\n",
    "        \n",
    "        # Show top layers\n",
    "        if layer_counts:\n",
    "            top_layers = sorted(layer_counts.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "            print(f\"    Most critical layers: {', '.join([f'{layer}({count})' for layer, count in top_layers])}\")\n",
    "    \n",
    "    ranking_results[metric] = topk_results\n",
    "\n",
    "print(f\"\\nâœ… Weight ranking complete for {len(CONFIG['sensitivity_metrics'])} metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b78a34",
   "metadata": {},
   "source": [
    "## ðŸ“Š Sensitivity Distribution Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259b0db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization of sensitivity distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('ðŸ”¬ Sensitivity Analysis Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "for i, metric in enumerate(CONFIG['sensitivity_metrics']):\n",
    "    layer_sensitivities = sensitivity_results[metric]\n",
    "    \n",
    "    # Plot 1: Distribution histogram\n",
    "    ax = axes[i, 0]\n",
    "    all_sens = np.concatenate([sens.cpu().numpy().flatten() \n",
    "                              for sens in layer_sensitivities.values()])\n",
    "    \n",
    "    # Use log scale for better visualization\n",
    "    log_sens = np.log10(all_sens + 1e-12)  # Add small epsilon to avoid log(0)\n",
    "    ax.hist(log_sens, bins=50, alpha=0.7, edgecolor='black')\n",
    "    ax.set_title(f'{metric.replace(\"_\", \" \").title()} Distribution (Log Scale)')\n",
    "    ax.set_xlabel('Log10(Sensitivity)')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Layer-wise sensitivity\n",
    "    ax = axes[i, 1]\n",
    "    layer_means = [layer_sensitivities[layer].mean().item() \n",
    "                   for layer in sorted(layer_sensitivities.keys())]\n",
    "    layer_names = sorted(layer_sensitivities.keys())\n",
    "    \n",
    "    # Show only every nth layer for readability\n",
    "    step = max(1, len(layer_names) // 10)\n",
    "    x_pos = range(0, len(layer_names), step)\n",
    "    x_labels = [layer_names[i].split('.')[-1] for i in x_pos]  # Short names\n",
    "    y_values = [layer_means[i] for i in x_pos]\n",
    "    \n",
    "    ax.bar(x_pos, y_values, alpha=0.7)\n",
    "    ax.set_title(f'{metric.replace(\"_\", \" \").title()} by Layer')\n",
    "    ax.set_xlabel('Layer')\n",
    "    ax.set_ylabel('Mean Sensitivity')\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(x_labels, rotation=45, ha='right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(session_dir / 'sensitivity_distributions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"ðŸ“Š Sensitivity distribution plots saved to {session_dir / 'sensitivity_distributions.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0934c1",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Perturbation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb85eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸŽ¯ Starting perturbation analysis...\")\n",
    "print(f\"ðŸ“Š Testing top-K values: {CONFIG['topk_values']}\")\n",
    "print(f\"ðŸ“Š Masking ratios: {CONFIG['perturbation_ratios']}\")\n",
    "\n",
    "perturbation_results = []\n",
    "\n",
    "# Test different metrics and top-K values\n",
    "for metric in CONFIG['sensitivity_metrics']:\n",
    "    print(f\"\\nðŸ”„ Perturbation analysis for {metric}...\")\n",
    "    \n",
    "    for k in CONFIG['topk_values']:\n",
    "        top_weights = ranking_results[metric][k]\n",
    "        print(f\"\\n  ðŸ“Š Top-{k} weights ({len(top_weights)} total)\")\n",
    "        \n",
    "        for ratio in CONFIG['perturbation_ratios']:\n",
    "            # Calculate how many weights to mask\n",
    "            num_to_mask = int(len(top_weights) * ratio)\n",
    "            if num_to_mask == 0:\n",
    "                continue\n",
    "                \n",
    "            weights_to_mask = top_weights[:num_to_mask]\n",
    "            \n",
    "            print(f\"    ðŸŽ¯ Masking {num_to_mask} weights ({ratio*100:.0f}%)...\", end=\" \")\n",
    "            \n",
    "            # Store original values\n",
    "            original_values = {}\n",
    "            \n",
    "            # Apply masking\n",
    "            try:\n",
    "                for weight_info in weights_to_mask:\n",
    "                    layer_name = weight_info['layer']\n",
    "                    indices = weight_info['indices']\n",
    "                    \n",
    "                    # Get the layer module\n",
    "                    layer = dict(model.named_modules())[layer_name]\n",
    "                    if hasattr(layer, 'weight') and layer.weight is not None:\n",
    "                        # Store original value\n",
    "                        key = (layer_name, tuple(indices))\n",
    "                        original_values[key] = layer.weight.data[tuple(indices)].clone()\n",
    "                        \n",
    "                        # Mask the weight (set to zero)\n",
    "                        layer.weight.data[tuple(indices)] = 0.0\n",
    "                \n",
    "                # Evaluate with masked weights\n",
    "                perturbed_ppl = compute_perplexity(model, tokenizer, eval_texts[:20])  # Use subset for speed\n",
    "                \n",
    "                # Restore original values\n",
    "                for weight_info in weights_to_mask:\n",
    "                    layer_name = weight_info['layer']\n",
    "                    indices = weight_info['indices']\n",
    "                    \n",
    "                    layer = dict(model.named_modules())[layer_name]\n",
    "                    if hasattr(layer, 'weight') and layer.weight is not None:\n",
    "                        key = (layer_name, tuple(indices))\n",
    "                        if key in original_values:\n",
    "                            layer.weight.data[tuple(indices)] = original_values[key]\n",
    "                \n",
    "                # Calculate impact\n",
    "                ppl_increase = perturbed_ppl - baseline_ppl\n",
    "                ppl_ratio = perturbed_ppl / baseline_ppl\n",
    "                \n",
    "                print(f\"PPL: {perturbed_ppl:.2f} (+{ppl_increase:.2f}, {ppl_ratio:.2f}x)\")\n",
    "                \n",
    "                # Store results\n",
    "                perturbation_results.append({\n",
    "                    'metric': metric,\n",
    "                    'topk': k,\n",
    "                    'mask_ratio': ratio,\n",
    "                    'weights_masked': num_to_mask,\n",
    "                    'baseline_ppl': baseline_ppl,\n",
    "                    'perturbed_ppl': perturbed_ppl,\n",
    "                    'ppl_increase': ppl_increase,\n",
    "                    'ppl_ratio': ppl_ratio\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                # Restore weights even if there was an error\n",
    "                for weight_info in weights_to_mask:\n",
    "                    layer_name = weight_info['layer']\n",
    "                    indices = weight_info['indices']\n",
    "                    \n",
    "                    layer = dict(model.named_modules())[layer_name]\n",
    "                    if hasattr(layer, 'weight') and layer.weight is not None:\n",
    "                        key = (layer_name, tuple(indices))\n",
    "                        if key in original_values:\n",
    "                            layer.weight.data[tuple(indices)] = original_values[key]\n",
    "\n",
    "print(f\"\\nâœ… Perturbation analysis complete: {len(perturbation_results)} experiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48957b2",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Results Analysis & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458fc90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame for analysis\n",
    "df = pd.DataFrame(perturbation_results)\n",
    "\n",
    "if len(df) > 0:\n",
    "    print(\"ðŸ“ˆ Perturbation Analysis Results:\")\n",
    "    print(f\"  Total experiments: {len(df)}\")\n",
    "    print(f\"  Metrics tested: {df['metric'].unique()}\")\n",
    "    print(f\"  Max PPL increase: {df['ppl_increase'].max():.2f}\")\n",
    "    print(f\"  Max PPL ratio: {df['ppl_ratio'].max():.2f}x\")\n",
    "    \n",
    "    # Show top results\n",
    "    print(\"\\nðŸ” Top 10 Most Impactful Perturbations:\")\n",
    "    top_results = df.nlargest(10, 'ppl_increase')\n",
    "    for _, row in top_results.iterrows():\n",
    "        print(f\"  {row['metric']}, Top-{row['topk']}, {row['mask_ratio']*100:.0f}% masked: \"\n",
    "              f\"PPL {row['baseline_ppl']:.1f} â†’ {row['perturbed_ppl']:.1f} \"\n",
    "              f\"(+{row['ppl_increase']:.1f})\")\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('ðŸŽ¯ Perturbation Analysis Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Plot 1: PPL increase by mask ratio\n",
    "    ax = axes[0, 0]\n",
    "    for metric in df['metric'].unique():\n",
    "        metric_data = df[df['metric'] == metric]\n",
    "        for k in sorted(metric_data['topk'].unique()):\n",
    "            k_data = metric_data[metric_data['topk'] == k]\n",
    "            ax.plot(k_data['mask_ratio'], k_data['ppl_increase'], \n",
    "                   marker='o', label=f'{metric}, Top-{k}')\n",
    "    \n",
    "    ax.set_xlabel('Masking Ratio')\n",
    "    ax.set_ylabel('Perplexity Increase')\n",
    "    ax.set_title('PPL Impact vs Masking Ratio')\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: PPL ratio by number of weights masked\n",
    "    ax = axes[0, 1]\n",
    "    scatter = ax.scatter(df['weights_masked'], df['ppl_ratio'], \n",
    "                        c=df['topk'], alpha=0.7, s=60)\n",
    "    ax.set_xlabel('Number of Weights Masked')\n",
    "    ax.set_ylabel('Perplexity Ratio')\n",
    "    ax.set_title('PPL Ratio vs Weights Masked')\n",
    "    plt.colorbar(scatter, ax=ax, label='Top-K Value')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Heatmap of PPL increases\n",
    "    ax = axes[1, 0]\n",
    "    pivot_data = df.pivot_table(values='ppl_increase', \n",
    "                               index='mask_ratio', \n",
    "                               columns='topk', \n",
    "                               aggfunc='mean')\n",
    "    sns.heatmap(pivot_data, annot=True, fmt='.1f', ax=ax, cmap='Reds')\n",
    "    ax.set_title('PPL Increase Heatmap\\n(Mask Ratio vs Top-K)')\n",
    "    \n",
    "    # Plot 4: Metric comparison\n",
    "    ax = axes[1, 1]\n",
    "    if len(df['metric'].unique()) > 1:\n",
    "        metric_comparison = df.groupby(['metric', 'mask_ratio'])['ppl_increase'].mean().unstack(level=0)\n",
    "        metric_comparison.plot(kind='bar', ax=ax)\n",
    "        ax.set_title('Metric Comparison by Mask Ratio')\n",
    "        ax.set_xlabel('Mask Ratio')\n",
    "        ax.set_ylabel('Mean PPL Increase')\n",
    "        ax.legend(title='Metric')\n",
    "    else:\n",
    "        # Show distribution if only one metric\n",
    "        ax.hist(df['ppl_increase'], bins=20, alpha=0.7, edgecolor='black')\n",
    "        ax.set_title('PPL Increase Distribution')\n",
    "        ax.set_xlabel('PPL Increase')\n",
    "        ax.set_ylabel('Frequency')\n",
    "    \n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(session_dir / 'perturbation_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Perturbation analysis plots saved to {session_dir / 'perturbation_analysis.png'}\")\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸ No perturbation results to analyze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff8e0b8",
   "metadata": {},
   "source": [
    "## ðŸ’¾ Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfca3397",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG['save_results']:\n",
    "    print(\"ðŸ’¾ Exporting results...\")\n",
    "    \n",
    "    # 1. Export perturbation results\n",
    "    if len(perturbation_results) > 0:\n",
    "        df.to_csv(session_dir / 'perturbation_results.csv', index=False)\n",
    "        print(f\"  âœ… Perturbation results: {session_dir / 'perturbation_results.csv'}\")\n",
    "    \n",
    "    # 2. Export top-K weights for each metric\n",
    "    for metric in CONFIG['sensitivity_metrics']:\n",
    "        for k in CONFIG['topk_values']:\n",
    "            if metric in ranking_results and k in ranking_results[metric]:\n",
    "                top_weights = ranking_results[metric][k]\n",
    "                \n",
    "                # Convert to DataFrame\n",
    "                weights_df = pd.DataFrame([\n",
    "                    {\n",
    "                        'rank': i + 1,\n",
    "                        'layer': w['layer'],\n",
    "                        'indices': str(w['indices']),\n",
    "                        'sensitivity': w['sensitivity']\n",
    "                    }\n",
    "                    for i, w in enumerate(top_weights)\n",
    "                ])\n",
    "                \n",
    "                filename = f'top_{k}_weights_{metric}.csv'\n",
    "                weights_df.to_csv(session_dir / filename, index=False)\n",
    "                print(f\"  âœ… Top-{k} {metric} weights: {session_dir / filename}\")\n",
    "    \n",
    "    # 3. Export experiment summary\n",
    "    summary = {\n",
    "        'experiment_id': CONFIG['timestamp'],\n",
    "        'model_name': CONFIG['model_name'],\n",
    "        'baseline_perplexity': baseline_ppl,\n",
    "        'num_parameters': sum(p.numel() for p in model.parameters()),\n",
    "        'eval_texts_count': len(eval_texts),\n",
    "        'sensitivity_metrics': CONFIG['sensitivity_metrics'],\n",
    "        'topk_values': CONFIG['topk_values'],\n",
    "        'perturbation_ratios': CONFIG['perturbation_ratios'],\n",
    "        'experiments_completed': len(perturbation_results),\n",
    "        'max_ppl_increase': df['ppl_increase'].max() if len(df) > 0 else None,\n",
    "        'max_ppl_ratio': df['ppl_ratio'].max() if len(df) > 0 else None\n",
    "    }\n",
    "    \n",
    "    summary_df = pd.DataFrame([summary])\n",
    "    summary_df.to_csv(session_dir / 'experiment_summary.csv', index=False)\n",
    "    print(f\"  âœ… Experiment summary: {session_dir / 'experiment_summary.csv'}\")\n",
    "    \n",
    "    # 4. Export configuration\n",
    "    config_df = pd.DataFrame(list(CONFIG.items()), columns=['parameter', 'value'])\n",
    "    config_df.to_csv(session_dir / 'config.csv', index=False)\n",
    "    print(f\"  âœ… Configuration: {session_dir / 'config.csv'}\")\n",
    "    \n",
    "    print(f\"\\nðŸŽ‰ All results exported to: {session_dir}\")\n",
    "    print(f\"ðŸ“ Files created:\")\n",
    "    for file in session_dir.glob('*'):\n",
    "        print(f\"  - {file.name}\")\n",
    "else:\n",
    "    print(\"ðŸ“„ Results export disabled in configuration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a9eb65",
   "metadata": {},
   "source": [
    "## ðŸ“‹ Analysis Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb84ae11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ”¬ Critical Weight Analysis - Complete Summary\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ðŸ“… Session: {CONFIG['timestamp']}\")\n",
    "print(f\"ðŸ¤– Model: {CONFIG['model_name']}\")\n",
    "print(f\"ðŸ“Š Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"ðŸ“š Evaluation texts: {len(eval_texts)}\")\n",
    "print(f\"ðŸ“ Baseline perplexity: {baseline_ppl:.2f}\")\n",
    "\n",
    "print(f\"\\nðŸ§® Sensitivity Analysis:\")\n",
    "for metric in CONFIG['sensitivity_metrics']:\n",
    "    layer_sensitivities = sensitivity_results[metric]\n",
    "    total_weights = sum(len(sens) for sens in layer_sensitivities.values())\n",
    "    all_sensitivities = np.concatenate([sens.cpu().numpy().flatten() \n",
    "                                       for sens in layer_sensitivities.values()])\n",
    "    print(f\"  {metric}:\")\n",
    "    print(f\"    Layers analyzed: {len(layer_sensitivities)}\")\n",
    "    print(f\"    Total weights: {total_weights:,}\")\n",
    "    print(f\"    Mean sensitivity: {all_sensitivities.mean():.2e}\")\n",
    "    print(f\"    Max sensitivity: {all_sensitivities.max():.2e}\")\n",
    "\n",
    "if len(perturbation_results) > 0:\n",
    "    print(f\"\\nðŸŽ¯ Perturbation Analysis:\")\n",
    "    print(f\"  Experiments completed: {len(perturbation_results)}\")\n",
    "    print(f\"  Max PPL increase: {df['ppl_increase'].max():.2f}\")\n",
    "    print(f\"  Max PPL ratio: {df['ppl_ratio'].max():.2f}x\")\n",
    "    \n",
    "    # Find most effective perturbation\n",
    "    best_result = df.loc[df['ppl_increase'].idxmax()]\n",
    "    print(f\"\\nðŸ† Most Impactful Perturbation:\")\n",
    "    print(f\"  Metric: {best_result['metric']}\")\n",
    "    print(f\"  Top-K: {best_result['topk']}\")\n",
    "    print(f\"  Mask ratio: {best_result['mask_ratio']*100:.0f}%\")\n",
    "    print(f\"  Weights masked: {best_result['weights_masked']}\")\n",
    "    print(f\"  PPL change: {best_result['baseline_ppl']:.1f} â†’ {best_result['perturbed_ppl']:.1f}\")\n",
    "    print(f\"  Impact: +{best_result['ppl_increase']:.1f} ({best_result['ppl_ratio']:.2f}x)\")\n",
    "\n",
    "print(f\"\\nðŸ’¾ Results saved to: {session_dir}\")\n",
    "print(f\"ðŸŽ‰ Analysis complete!\")\n",
    "\n",
    "# Key insights\n",
    "print(f\"\\nðŸ”‘ Key Insights:\")\n",
    "if len(perturbation_results) > 0:\n",
    "    print(f\"  â€¢ Masking top-{best_result['topk']} {best_result['metric']} weights causes {best_result['ppl_increase']:.1f} PPL increase\")\n",
    "    print(f\"  â€¢ Most critical weights concentrate in specific layers\")\n",
    "    print(f\"  â€¢ {best_result['metric'].replace('_', ' ').title()} shows strongest sensitivity correlation\")\n",
    "else:\n",
    "    print(f\"  â€¢ Sensitivity analysis completed successfully\")\n",
    "    print(f\"  â€¢ Weight ranking identified critical parameters\")\n",
    "    print(f\"  â€¢ Ready for perturbation experiments\")\n",
    "\n",
    "print(f\"\\nðŸš€ Next Steps:\")\n",
    "print(f\"  â€¢ Analyze layer-specific sensitivity patterns\")\n",
    "print(f\"  â€¢ Test different masking strategies\")\n",
    "print(f\"  â€¢ Explore attention head importance\")\n",
    "print(f\"  â€¢ Compare across different model sizes\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
