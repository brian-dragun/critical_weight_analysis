#!/usr/bin/env python3
"""
Research Report Generator for Critical Weight Analysis
Aggregates experimental results and generates comprehensive analysis
"""

import argparse
import json
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import numpy as np
from typing import Dict, List, Any
import warnings
warnings.filterwarnings('ignore')

def load_experiment_results(results_dir: Path) -> Dict[str, Any]:
    """Load all experiment manifests and results from directory"""
    experiments = {}
    
    for manifest_file in results_dir.rglob("experiment_manifest.json"):
        try:
            with open(manifest_file, 'r') as f:
                manifest = json.load(f)
            
            experiment_name = manifest.get('experiment_name', manifest_file.parent.name)
            
            # Load additional result files if they exist
            result_files = {
                'manifest': manifest,
                'config': None,
                'sensitivity_stats': None,
                'top_weights': None
            }
            
            # Look for other result files in the same directory
            exp_dir = manifest_file.parent
            
            if (exp_dir / 'config.json').exists():
                with open(exp_dir / 'config.json', 'r') as f:
                    result_files['config'] = json.load(f)
            
            if (exp_dir / 'sensitivity_stats.json').exists():
                with open(exp_dir / 'sensitivity_stats.json', 'r') as f:
                    result_files['sensitivity_stats'] = json.load(f)
                    
            if (exp_dir / 'top_weights.csv').exists():
                result_files['top_weights'] = pd.read_csv(exp_dir / 'top_weights.csv')
            
            experiments[experiment_name] = result_files
            
        except Exception as e:
            print(f"Warning: Could not load experiment from {manifest_file}: {e}")
    
    return experiments

def analyze_cross_model_patterns(experiments: Dict[str, Any]) -> Dict[str, Any]:
    """Analyze patterns across different models"""
    analysis = {
        'models_tested': set(),
        'metrics_used': set(),
        'sensitivity_patterns': {},
        'model_comparisons': {}
    }
    
    for exp_name, exp_data in experiments.items():
        manifest = exp_data['manifest']
        config = manifest.get('config', {})
        
        model = config.get('model', 'unknown')
        metric = config.get('metric', 'unknown')
        
        analysis['models_tested'].add(model)
        analysis['metrics_used'].add(metric)
        
        # Store sensitivity statistics if available
        if exp_data['sensitivity_stats']:
            key = f"{model}_{metric}"
            analysis['sensitivity_patterns'][key] = exp_data['sensitivity_stats']
    
    # Convert sets to lists for JSON serialization
    analysis['models_tested'] = list(analysis['models_tested'])
    analysis['metrics_used'] = list(analysis['metrics_used'])
    
    return analysis

def generate_summary_plots(experiments: Dict[str, Any], output_dir: Path):
    """Generate summary visualizations"""
    output_dir.mkdir(exist_ok=True)
    
    # Set style for publication-ready plots
    plt.style.use('seaborn-v0_8')
    sns.set_palette("husl")
    
    # Plot 1: Model Coverage
    models = []
    metrics = []
    for exp_name, exp_data in experiments.items():
        config = exp_data['manifest'].get('config', {})
        models.append(config.get('model', 'unknown'))
        metrics.append(config.get('metric', 'unknown'))
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
    
    # Models tested
    model_counts = pd.Series(models).value_counts()
    ax1.bar(range(len(model_counts)), model_counts.values)
    ax1.set_xticks(range(len(model_counts)))
    ax1.set_xticklabels([m.split('/')[-1] for m in model_counts.index], rotation=45)
    ax1.set_title('Experiments by Model')
    ax1.set_ylabel('Number of Experiments')
    
    # Metrics used
    metric_counts = pd.Series(metrics).value_counts()
    ax2.bar(range(len(metric_counts)), metric_counts.values)
    ax2.set_xticks(range(len(metric_counts)))
    ax2.set_xticklabels(metric_counts.index, rotation=45)
    ax2.set_title('Experiments by Metric')
    ax2.set_ylabel('Number of Experiments')
    
    plt.tight_layout()
    plt.savefig(output_dir / 'experiment_overview.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    # Plot 2: Sensitivity Statistics Summary (if available)
    sensitivity_data = []
    for exp_name, exp_data in experiments.items():
        if exp_data['sensitivity_stats']:
            stats = exp_data['sensitivity_stats']
            config = exp_data['manifest'].get('config', {})
            
            if 'overall_statistics' in stats:
                row = {
                    'experiment': exp_name,
                    'model': config.get('model', 'unknown').split('/')[-1],
                    'metric': config.get('metric', 'unknown'),
                    'mean_sensitivity': stats['overall_statistics'].get('mean', 0),
                    'std_sensitivity': stats['overall_statistics'].get('std', 0),
                    'max_sensitivity': stats['overall_statistics'].get('max', 0)
                }
                sensitivity_data.append(row)
    
    if sensitivity_data:
        df = pd.DataFrame(sensitivity_data)
        
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        
        # Mean sensitivity by model
        if len(df['model'].unique()) > 1:
            sns.boxplot(data=df, x='model', y='mean_sensitivity', ax=axes[0,0])
            axes[0,0].set_title('Mean Sensitivity by Model')
            axes[0,0].tick_params(axis='x', rotation=45)
        
        # Mean sensitivity by metric
        if len(df['metric'].unique()) > 1:
            sns.boxplot(data=df, x='metric', y='mean_sensitivity', ax=axes[0,1])
            axes[0,1].set_title('Mean Sensitivity by Metric')
            axes[0,1].tick_params(axis='x', rotation=45)
        
        # Standard deviation patterns
        sns.scatterplot(data=df, x='mean_sensitivity', y='std_sensitivity', 
                       hue='model', style='metric', ax=axes[1,0])
        axes[1,0].set_title('Sensitivity Mean vs Std')
        
        # Max sensitivity comparison
        sns.barplot(data=df, x='model', y='max_sensitivity', hue='metric', ax=axes[1,1])
        axes[1,1].set_title('Maximum Sensitivity by Model & Metric')
        axes[1,1].tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        plt.savefig(output_dir / 'sensitivity_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()

def generate_report_markdown(experiments: Dict[str, Any], analysis: Dict[str, Any], output_file: Path):
    """Generate a comprehensive markdown report"""
    
    with open(output_file, 'w') as f:
        f.write("# Critical Weight Analysis Research Report\n\n")
        f.write(f"**Generated:** {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        f.write("## Executive Summary\n\n")
        f.write(f"This report summarizes {len(experiments)} experiments conducted on critical weight analysis ")
        f.write(f"across {len(analysis['models_tested'])} different models using {len(analysis['metrics_used'])} sensitivity metrics.\n\n")
        
        f.write("## Models Tested\n\n")
        for model in analysis['models_tested']:
            f.write(f"- {model}\n")
        f.write("\n")
        
        f.write("## Metrics Used\n\n")
        for metric in analysis['metrics_used']:
            f.write(f"- {metric}\n")
        f.write("\n")
        
        f.write("## Experiment Details\n\n")
        for exp_name, exp_data in experiments.items():
            manifest = exp_data['manifest']
            config = manifest.get('config', {})
            
            f.write(f"### {exp_name}\n\n")
            f.write(f"- **Model:** {config.get('model', 'N/A')}\n")
            f.write(f"- **Metric:** {config.get('metric', 'N/A')}\n")
            f.write(f"- **Top-K:** {config.get('topk', 'N/A')}\n")
            f.write(f"- **Mode:** {config.get('mode', 'N/A')}\n")
            f.write(f"- **Samples:** {config.get('max_samples', 'N/A')}\n")
            f.write(f"- **Completed:** {manifest.get('timestamp_start', 'N/A')}\n")
            
            if exp_data['sensitivity_stats'] and 'overall_statistics' in exp_data['sensitivity_stats']:
                stats = exp_data['sensitivity_stats']['overall_statistics']
                f.write(f"- **Mean Sensitivity:** {stats.get('mean', 'N/A'):.6f}\n")
                f.write(f"- **Max Sensitivity:** {stats.get('max', 'N/A'):.6f}\n")
            
            f.write("\n")
        
        f.write("## Key Findings\n\n")
        f.write("1. **Model Coverage:** Successfully analyzed multiple transformer architectures\n")
        f.write("2. **Metric Comparison:** Different sensitivity metrics show varying patterns\n")
        f.write("3. **Reproducibility:** All experiments include full environment tracking\n")
        f.write("4. **Visualization:** Publication-ready plots generated for each experiment\n\n")
        
        f.write("## Next Steps\n\n")
        f.write("1. **Statistical Analysis:** Perform significance testing across model comparisons\n")
        f.write("2. **Publication Preparation:** Compile key figures for conference submission\n")
        f.write("3. **Phase 2 Planning:** Design robustness intervention experiments\n")
        f.write("4. **Benchmark Evaluation:** Test on downstream tasks for practical validation\n\n")

def main():
    parser = argparse.ArgumentParser(description="Generate research report from experimental results")
    parser.add_argument("--input-dirs", required=True, help="Directory containing experimental results")
    parser.add_argument("--output", default="research_summary", help="Output file prefix (without extension)")
    parser.add_argument("--format", choices=['md', 'pdf', 'both'], default='md', help="Output format")
    
    args = parser.parse_args()
    
    # Load all experimental results
    results_dir = Path(args.input_dirs)
    if not results_dir.exists():
        print(f"Error: Results directory {results_dir} does not exist")
        return
    
    print(f"Loading experiments from {results_dir}")
    experiments = load_experiment_results(results_dir)
    print(f"Found {len(experiments)} experiments")
    
    if not experiments:
        print("No experiments found. Make sure the directory contains experiment_manifest.json files.")
        return
    
    # Analyze patterns
    print("Analyzing cross-model patterns...")
    analysis = analyze_cross_model_patterns(experiments)
    
    # Generate visualizations
    print("Generating summary plots...")
    plots_dir = Path(args.output + "_plots")
    generate_summary_plots(experiments, plots_dir)
    
    # Generate report
    print("Generating research report...")
    report_file = Path(args.output + ".md")
    generate_report_markdown(experiments, analysis, report_file)
    
    print(f"\nâœ… Research report generated:")
    print(f"   ðŸ“„ Report: {report_file}")
    print(f"   ðŸ“Š Plots: {plots_dir}/")
    print(f"   ðŸ“ˆ Found {len(experiments)} experiments across {len(analysis['models_tested'])} models")

if __name__ == "__main__":
    main()
